{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Boosting Techniques"
      ],
      "metadata": {
        "id": "mAuZIwVXLpAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "\n",
        "Ans- Boosting in machine learning is an ensemble technique that combines multiple weak learners (models that perform slightly better than random) into a strong learner. It works sequentially, where each new learner focuses on the errors of the previous ones. By giving more weight to misclassified data points and combining all weak models, boosting reduces bias, improves accuracy, and turns weak learners into a powerful predictive model."
      ],
      "metadata": {
        "id": "Ei0zVEKZLt4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "\n",
        "Ans- AdaBoost trains weak learners sequentially by adjusting the weights of data points, giving more weight to misclassified samples.\n",
        "\n",
        "Gradient Boosting trains weak learners by fitting them to the residual errors (gradients of the loss function) from the previous model."
      ],
      "metadata": {
        "id": "YAa98BFZMHWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  How does regularization help in XGBoost?\n",
        "\n",
        "Ans- Regularization in XGBoost helps by penalizing model complexity (too many trees or overly deep trees), which prevents overfitting. It controls both the weights of leaf nodes (L1 & L2 regularization) and the depth/number of trees, leading to better generalization and more robust predictions."
      ],
      "metadata": {
        "id": "gSmBfU21MYCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "Ans- CatBoost is efficient for handling categorical data because it uses ordered target statistics and permutation-based encoding instead of one-hot encoding. This reduces data sparsity, avoids overfitting, and allows the model to process categorical features directly and efficiently."
      ],
      "metadata": {
        "id": "tyziDwGzM8IP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "\n",
        "Ans- Boosting is preferred over bagging in applications where high accuracy and complex pattern detection are needed, such as:\n",
        "\n",
        "Credit scoring & fraud detection (finance)\n",
        "\n",
        "Customer churn prediction (telecom, marketing)\n",
        "\n",
        "Medical diagnosis (healthcare)\n",
        "\n",
        "Search ranking & recommendations (Google, YouTube, e-commerce)\n",
        "\n",
        "Risk modeling & insurance"
      ],
      "metadata": {
        "id": "dQdQN9oHNMZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Write a Python program to:\n",
        "\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy"
      ],
      "metadata": {
        "id": "xgbiUfGANXaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"AdaBoost Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XOChfSQNljR",
        "outputId": "30f84450-b1e2-4adf-dcd7-2cb5127d5b37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score"
      ],
      "metadata": {
        "id": "zCBfiQJqNxiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Gradient Boosting Regressor R-squared Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW2jf1K_N7vx",
        "outputId": "ec01987e-6543-4b63-b6a9-9228b46f1f2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared Score: 0.8004451261281281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  Write a Python program to:\n",
        "\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "L54DYDIAOBcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define model\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define parameter grid for learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# GridSearchCV for tuning\n",
        "grid = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"XGBoost Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L26V_up_OKLC",
        "outputId": "90448cc2-03b3-4baf-b8d8-e4bb56e2cfe8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "XGBoost Classifier Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [15:10:18] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. : Write a Python program to:\n",
        "\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "bbxRQLczOPI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target  # 0 = malignant, 1 = benign (same order as target_names)\n",
        "\n",
        "# 2) Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3) Train CatBoost (binary logloss for clear probabilistic outputs)\n",
        "model = CatBoostClassifier(\n",
        "    loss_function=\"Logloss\",\n",
        "    iterations=300,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4) Predict robustly as class labels\n",
        "# (avoid shape/type issues by thresholding probabilities explicitly)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]        # P(class=1)\n",
        "y_pred = (y_proba >= 0.5).astype(int)              # convert to 0/1 ints\n",
        "\n",
        "# 5) Accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"CatBoost Accuracy: {acc:.4f}\")\n",
        "\n",
        "# 6) Confusion matrix with fixed label order to match target_names\n",
        "labels = [0, 1]  # 0=malignant, 1=benign\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "\n",
        "# 7) Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(\n",
        "    cm, annot=True, fmt='d', cbar=False,\n",
        "    xticklabels=data.target_names,  # ['malignant','benign']\n",
        "    yticklabels=data.target_names\n",
        ")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "MIB78IWOPGfm",
        "outputId": "3dbb53b4-66e6-409b-ccfa-15de0d57adf5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-503793365.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_breast_cancer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "\n",
        "Ans- 1) Data preprocessing & handling missing / categorical values\n",
        "\n",
        "a. Quick data audit\n",
        "\n",
        "Check class balance, missingness by column, cardinality of categorical features, skewness, outliers.\n",
        "\n",
        "b. Missing values\n",
        "\n",
        "If using CatBoost or XGBoost: they natively handle missing values — you can leave NaNs for the model to use.\n",
        "\n",
        "If you prefer explicit imputation (for analysis or other models):\n",
        "\n",
        "Numeric: median (robust) or KNN/Iterative imputer for complex patterns.\n",
        "\n",
        "Categorical: new category \"MISSING\" or frequency-based imputation.\n",
        "\n",
        "Log missingness as a separate binary feature for important columns (helps model).\n",
        "\n",
        "c. Categorical features\n",
        "\n",
        "CatBoost: pass raw categorical columns (no encoding).\n",
        "\n",
        "XGBoost / AdaBoost: encode:\n",
        "\n",
        "Low-cardinality: one-hot / target encoding with careful leakage control (use CV-based/leave-one-out or mean/impact encoding with smoothing).\n",
        "\n",
        "High-cardinality: target encoding, hashing, or embeddings. Use ordered/regularized target stats to avoid leakage.\n",
        "\n",
        "Always use encodings computed only on training folds (no target leakage).\n",
        "\n",
        "d. Numeric features\n",
        "\n",
        "Trees don’t require scaling; consider transformations (log) for heavy skew; create interaction/ratio features if domain-relevant.\n",
        "\n",
        "e. Imbalance handling\n",
        "\n",
        "Preferred (for boosting):\n",
        "\n",
        "Use class weights / scale_pos_weight (XGBoost) / class_weights where available.\n",
        "\n",
        "Use stratified CV to preserve class ratios.\n",
        "\n",
        "If needed, combine with sampling: SMOTE (on training folds only) or under-sampling majority + boosting.\n",
        "\n",
        "Consider optimizing on a cost-sensitive loss (or custom objective) that reflects business cost of FN vs FP.\n",
        "\n",
        "f. Feature selection / engineering\n",
        "\n",
        "Remove leaky features. Create aggregates from transaction history (e.g., avg balance, volatility, recent delinquencies). Use domain rules (e.g., overdue ratios).\n",
        "\n",
        "Check for multicollinearity (not critical for trees) and drop redundant features if helpful.\n",
        "\n",
        "2) Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "CatBoost — recommended if you have many categorical features and need fast, robust handling of categorical variables and fewer encoding steps. It also handles missing values and tends to be robust to default hyperparameters.\n",
        "\n",
        "XGBoost — choose it if you need highly optimized performance, fine-grained control, or distributed training. Use when features are mostly numeric or you’re comfortable encoding categoricals safely.\n",
        "\n",
        "AdaBoost — less preferred for tabular tasks with complex feature interactions and imbalance; simpler but usually outperformed by the gradient-boosting implementations.\n",
        "\n",
        "(Overall: CatBoost → first choice for mixed categorical/numeric; XGBoost → second choice when you want max speed/ops control; AdaBoost for simple baselines.)\n",
        "\n",
        "3) Hyperparameter tuning strategy\n",
        "\n",
        "a. Validation strategy\n",
        "\n",
        "Use Stratified K-Fold CV (e.g., 5 folds) or time-based split if data is time-ordered (use rolling splits). Optimize using CV to avoid leakage.\n",
        "\n",
        "Use nested CV for unbiased performance estimates if model selection is critical.\n",
        "\n",
        "b. Search approach\n",
        "\n",
        "Randomized search / Bayesian optimization (Optuna) to explore broad ranges quickly.\n",
        "\n",
        "Refined grid search around the best region or continue Bayesian for efficiency.\n",
        "\n",
        "Use early stopping on a validation fold to avoid overfitting (early_stopping_rounds).\n",
        "\n",
        "c. Parameters to tune (example ranges)\n",
        "\n",
        "XGBoost / CatBoost common:\n",
        "\n",
        "learning_rate (eta): 0.01 – 0.3\n",
        "\n",
        "n_estimators / iterations: 100 – 2000 (use early stopping)\n",
        "\n",
        "max_depth: 3 – 10\n",
        "\n",
        "min_child_weight (XGBoost) / min_data_in_leaf (CatBoost): 1 – 50\n",
        "\n",
        "subsample / rsm (feature fraction): 0.5 – 1.0\n",
        "\n",
        "colsample_bytree: 0.4 – 1.0\n",
        "\n",
        "Regularization: reg_alpha (L1) 0 – 1, reg_lambda (L2) 0 – 10\n",
        "\n",
        "For imbalance: scale_pos_weight = (n_negative / n_positive) as starting point (tune around it)\n",
        "\n",
        "CatBoost specific: border_count, l2_leaf_reg, bagging_temperature.\n",
        "\n",
        "d. Scoring during tuning\n",
        "\n",
        "Optimize for business-relevant metric (see next). Avoid optimizing raw accuracy when data is imbalanced.\n",
        "\n",
        "e. Reproducibility\n",
        "\n",
        "Fix random seed; log experiments (MLflow, Weights & Biases); save best model.\n",
        "\n",
        "4) Evaluation metrics (and why)\n",
        "\n",
        "Because the dataset is imbalanced, prefer metrics that reflect performance on the minority class and business cost:\n",
        "\n",
        "Primary metrics\n",
        "\n",
        "Precision-Recall AUC (PR-AUC) — better than ROC-AUC when positive class is rare; focuses on classifier’s performance for the positive (default) class.\n",
        "\n",
        "Recall (Sensitivity) — if missing a default (false negative) is costly, maximize recall (possibly at controlled precision).\n",
        "\n",
        "Precision@k / Top-k Recall / Lift — practical for actioning a top-k list of risky loans (useful for targeted interventions).\n",
        "\n",
        "Cost-sensitive metric / Expected monetary loss — compute expected loss using business costs of FN and FP; optimize for profit or minimized expected loss.\n",
        "\n",
        "Secondary metrics\n",
        "\n",
        "ROC-AUC — general discrimination ability (still informative).\n",
        "\n",
        "F1-score — balance between precision and recall (useful when both matter).\n",
        "\n",
        "Confusion matrix — interpret absolute counts.\n",
        "\n",
        "Calibration curve / Brier score — for reliability of probabilities (important if you’ll use probabilities for scoring/pricing).\n",
        "\n",
        "Model explainability & stability\n",
        "\n",
        "SHAP values — to explain individual predictions and global feature importance (useful for compliance and stakeholder trust).\n",
        "\n",
        "Stability over time / Backtesting — monitor drift and degradation.\n",
        "\n",
        "5) How the business benefits from the model\n",
        "\n",
        "Reduced financial loss: Identify high-risk borrowers early, reduce defaults via interventions (restructuring, collections), and avoid issuing loans to unsustainably risky applicants.\n",
        "\n",
        "Better capital allocation & pricing: Use risk scores for risk-based pricing and capital provisioning—higher precision reduces unnecessary rejections and optimizes revenue.\n",
        "\n",
        "Targeted collections: Prioritize collection efforts toward high-likelihood defaulters (higher ROI on collections).\n",
        "\n",
        "Operational efficiency: Automate decisioning for low-risk cases, freeing manual underwriting for borderline/high-complexity cases.\n",
        "\n",
        "Regulatory & auditability: Use explainability tools (SHAP, feature importance) to provide rationale for decisions and meet audit/regulatory requirements.\n",
        "\n",
        "Actionable segments: Create segments (e.g., “high risk, high recovery probability”) to customize retention or recovery strategies.\n",
        "\n",
        "6) Deployment & monitoring (short)\n",
        "\n",
        "Threshold selection: Choose probability threshold based on business cost matrix, not default 0.5.\n",
        "\n",
        "A/B test / Shadow deployment: Validate model impact on decision metrics and business KPIs before full rollout.\n",
        "\n",
        "Monitoring: Track model performance (PR-AUC, recall), data drift, features distribution, and business KPIs (default rate, loss). Retrain on schedule or when drift detected.\n",
        "\n",
        "Governance: Keep model registry, versioned datasets, and retraining policy.\n",
        "\n",
        "Quick practical checklist (action items)\n",
        "\n",
        "EDA & missingness report → create missing flags.\n",
        "\n",
        "Feature engineering (transaction aggregates).\n",
        "\n",
        "Choose CatBoost if many categoricals; else XGBoost.\n",
        "\n",
        "Use stratified CV; optimize PR-AUC / cost metric via Bayesian search; enable early stopping.\n",
        "\n",
        "Evaluate with PR-AUC, recall, precision@k and compute expected monetary impact.\n",
        "\n",
        "Explain predictions with SHAP and test in a shadow environment.\n",
        "\n",
        "Deploy with monitoring, threshold tuned to business costs."
      ],
      "metadata": {
        "id": "csghMMRWPJef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('loan_data.csv')\n",
        "\n",
        "# Identify features\n",
        "cat_features = df.select_dtypes(include='object').columns.tolist()\n",
        "num_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "target = 'loan_default'\n",
        "\n",
        "# Handle missing values\n",
        "for col in num_features:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "for col in cat_features:\n",
        "    df[col].fillna('Unknown', inplace=True)\n",
        "\n",
        "# Train-test split\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "# CatBoost Pool\n",
        "train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
        "test_pool = Pool(X_test, y_test, cat_features=cat_features)\n",
        "\n",
        "# Model initialization\n",
        "model = CatBoostClassifier(verbose=0, class_weights=[1, 5])  # Assuming class 1 is minority\n",
        "\n",
        "# Hyperparameter tuning\n",
        "params = {\n",
        "    'depth': [4, 6, 8],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'iterations': [100, 300, 500],\n",
        "    'l2_leaf_reg': [1, 3, 5]\n",
        "}\n",
        "\n",
        "grid = RandomizedSearchCV(model, param_distributions=params, cv=3, scoring='roc_auc', n_iter=10, random_state=42)\n",
        "grid.fit(X_train, y_train, cat_features=cat_features)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "rtvtnB4cRcHO",
        "outputId": "94e33048-fd5e-49f9-94e0-c90ddde3f485"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-457267673.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}